---
title: "VPD Crime Data Exploration"
author: "Zelalem Araya (92797935) & Youjung Kim (3876269)"
date: "2024-04-25"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r moresetup}

suppressPackageStartupMessages(require(rstan))
suppressPackageStartupMessages(require(dplyr))
suppressPackageStartupMessages(require(ggplot2))
suppressPackageStartupMessages(require(magrittr))
suppressPackageStartupMessages(require(tidybayes))
suppressPackageStartupMessages(require(ggmap))
suppressPackageStartupMessages(require(sf))
suppressPackageStartupMessages(require(sp))
suppressPackageStartupMessages(require(leaflet))
suppressPackageStartupMessages(require(geosphere))
suppressPackageStartupMessages(require(fields))
```

```{r}

crime_data <- read.csv("VanCrimeData2022.csv")
crime_data <- na.omit(crime_data)
crime_data$longitude <- crime_data$X
crime_data$latitude <- crime_data$Y
crime_data_all <- crime_data
crime_data_huh <- crime_data[c(101:200),]
crime_data <- crime_data[-c(101:nrow(crime_data)), ]
crime_data$BINMONTH <- as.numeric(crime_data$MONTH > 6)
tail(crime_data_all)


```

```{r}
#data exploration 

ggplot(data = crime_data_all)+
  geom_bar(aes(x = MONTH))

ggplot(data = crime_data_all)+
  geom_bar(aes(y = TYPE))
```

```{r}

crime_data_sf_all <- st_as_sf(crime_data_all, coords = c("longitude", "latitude"), crs = 26710) # 26710 code for UTM10
crime_data_sf_all <- st_transform(crime_data_sf_all, crs = 4326) # Convert to WGS84

crime_data_sf <- st_as_sf(crime_data, coords = c("longitude", "latitude"), crs = 26710) # 26710 code for UTM10
crime_data_sf <- st_transform(crime_data_sf, crs = 4326) # Convert to WGS84

crime_data_sf_huh <- st_as_sf(crime_data_huh, coords = c("longitude", "latitude"), crs = 26710) # 26710 code for UTM10

crime_data_sf_huh <- st_transform(crime_data_sf_huh, crs = 4326) # Convert to WGS84
```

```{r}

# Define the bounding box for Vancouver (example coordinates, adjust accordingly)
vancouver_bbox <- st_bbox(c(xmin = -123.2240, xmax = -123.0232, ymin = 49.1980, ymax = 49.3160), crs = st_crs(4326))

# Convert the bounding box to an sf polygon
vancouver_area <- st_as_sfc(vancouver_bbox)

# Define grid size - smaller values result in a finer grid
cellsize <- 0.01  # Degree units; adjust based on desired resolution

# Create a grid over the area
vancouver_grid <- st_make_grid(vancouver_area, cellsize = cellsize, square = TRUE)

# Convert the grid to a simple feature collection for easy manipulation
vancouver_grid_sf <- st_sf(geometry = vancouver_grid)

```

```{r}
area <- read_sf(dsn = "local-area-boundary/local-area-boundary.shp" )

```

```{r}
library(ggplot2)

ggplot() +
  geom_sf(data = area)+
  geom_sf(data = crime_data_sf) +
  ggtitle("Crime in Vancouver")

```

```{stan output.var = "geo_model"}
data {
  int<lower=0> N; // number of data points (train)
  vector[N] x; // long of train
  vector[N] y; // lat of train
  vector[N] crimes; // count of train
  
  int<lower=0> N_new; // number of data points (test)
  vector[N_new] x_new; // long of test
  vector[N_new] y_new; //lat of test
}

parameters {
  real alpha; //intercept
  real<lower=0> rho; //range
  real<lower=0> sigma; //spatial variance
  real<lower=0> sigma_err; // measurement error
}

model {
  vector[N] mu;
  
  for (i in 1:N) {
    mu[i] = alpha;
  }
  
  // Priors
  alpha ~ normal(0, 10);
  rho ~ inv_gamma(2, 1);
  sigma ~ normal(0, 1);
  sigma_err ~ normal(0, 1);

  // Spatial structure
  {
    matrix[N, N] dist;
    for (i in 1:N) {
      for (j in 1:N) {
        dist[i, j] = sqrt(square(x[i] - x[j]) + square(y[i] - y[j]));
      }
    }
    crimes ~ multi_normal(mu, sigma * exp(-dist/rho) + diag_matrix(rep_vector(sigma_err, N)));
  }
}

generated quantities{
  vector[N_new] pred_crimes; //predicted crimes
  matrix[N, N_new] new_dist; //distance from old to new
  for (i in 1:N){
    for (j in 1:N_new){
      new_dist[i, j] = exp(-sqrt(square(x[i] - x_new[j]) + square(y[i] - y_new[j])) / rho);
    }
  }
  
  for (i in 1:N_new){
    pred_crimes[i] = normal_rng(alpha + dot_product(new_dist[, i], crimes - rep_vector(alpha, N)), sigma_err);
  }
}
  
```

```{stan output.var = "canstangeo"}
data {
  int<lower=0> N; // Number of observed data points
  vector[N] y;    // Observed values
  matrix[N, N] distance_matrix; // Matrix of distances between data points
}

parameters {
  real mu; // Mean of the underlying spatial process
  real<lower=0> nugget; // Nugget effect
  real<lower=0> range;  // Range parameter for the spatial correlation
  real<lower=0> sill;   // Sill parameter (total variance)
}

model {
  // Priors for the mean and spatial parameters
  mu ~ normal(0, 1000);
  nugget ~ inv_gamma(0.1, 0.1);
  range ~ normal(0, 1000);
  sill ~ inv_gamma(0.1, 0.1);
  
  // Spatial correlation matrix based on the exponential model
  matrix[N, N] Sigma;
  for (i in 1:N) {
    for (j in i:N) { // The matrix is symmetric
      Sigma[i, j] = sill * exp(-distance_matrix[i, j] / range);
      if (i == j) {
        Sigma[i, j] += nugget; // Adding the nugget effect to the diagonal
      } else {
        Sigma[j, i] = Sigma[i, j]; // Ensuring the matrix is symmetric
      }
    }
  }
  
  // The likelihood of the observed data
  y ~ multi_normal(rep_vector(mu, N), Sigma);
}

generated quantities {
  vector[N2] y_pred;
  for (i in 1:N2) y_pred[i] = inv_logit(beta+y2[i]);
}
// generated quant ifelse

```

```{stan output.var = canstangeopredict}
data {
  int<lower=0> N1; // Number of data points
  int x1[N1];    // number of outcomes
  int n1[N1]; //number of observations
  int<lower=1> N2; // number of new points
  matrix[N1+N2, N1+N2] distance_matrix; // distances btwn points
}

transformed data{
  int<lower=1> N;
  N = N1 + N2;
}

parameters {
  vector[N1] y1; //no clue
  vector[N2] y2; //no clue
  real beta; // no clue
  real sigma_sq;// no clue
  real phi; //no clue
}

transformed parameters{
  vector[N1+N2] mu; //underlying spatial process mean
  for (i in 1:N) mu[i] = beta;
}

model {
  vector[N] y;
  matrix[N,N] Sigma;
  matrix [N,N] L;
  
  for (i in 1:N1) y[i] = y1[i];
  for (i in 1:N2) y[N1+i] = y2[i];
  for (i in 1:(N-1)){
    for (j in (i+1):N){
      Sigma[i,j] = exp((-1)*phi*distance_matrix[i,j]);
      Sigma[j,i] = Sigma[i,j]; //symmetric?
  }
}

  for (i in 1:N) Sigma[i,i] = sigma_sq;
  L = cholesky_decompose(Sigma);
  sigma_sq ~ normal(0,5);
  phi ~ normal(0, 5);
  y ~ multi_normal_cholesky(mu, L);
  beta ~ normal(0,5);
  x1 ~ binomial_logit(n1, y1);
}

generated quantities {
  vector[N2] y_pred;
  for (i in 1:N2) y_pred[i] = inv_logit(beta+y2[i]);
}

```

```{r}
# Extract the coordinates from your sf object
coords <- st_coordinates(crime_data_sf)

# Calculate the distance matrix
dist_matrix <- distm(coords, coords, fun = distHaversine)

# distance_matrix is a matrix where each element [i, j] represents the distance
# between the i-th and j-th points in your dataset, in meters.

```

```{r}

# Assuming you have your distance matrix and crime_data prepared
stan_data <- list(N = nrow(crime_data),
                  y = crime_data$MONTH, 
                  distance_matrix = dist_matrix)

```



```{r message=FALSE, warning=FALSE, results=FALSE, dependson=knitr::dep_prev()}

fit = sampling(
  geo_model,
  data = stan_data,
  chains = 1,
  refresh = 0,
  iter = 2000
)

print(fit)
```

```{r}
samples <- rstan::extract(fit)
```


```{r leave-one-out_1}
library(rstan)

# Function to perform LOOCV
perform_loocv <- function(data, model_file) {
  N <- nrow(data)
  loo_predictions <- numeric(N)
  
  for (i in 1:N) {
    # Create training/testing data excluding/including the i-th point
    train_data <- data[-i, ]
    test_data <- data[i, ]
    
    # Run the Stan model with training and testing data
    stan_data <- list(
      N = nrow(train_data),
      x = train_data$logitude,
      y = train_data$latitude,
      crimes = nrow(stan_data),
      N_new = nrow(test_data),
      x_new = test_data$logitude,
      y_new = test_data$latitude
    )
    
    # Fit the model
    fit <- sampling(geo_model,
                data = stan_data, 
                chains = 4,
                iter = 1000)
    
    # Extract predictions for the i-th point
    loo_predictions[i] <- as.numeric(fit$summary("pred_crimes")[1, "mean"])
  }
  
  return(loo_predictions)
}

# Group the data by NEIGHBOURHOOD
grouped_data <- split(crime_data, crime_data$NEIGHBOURHOOD)

# Initialize vector to store LOO predictions
loo_predictions <- numeric(0)

# Loop over each group (NEIGHBOURHOOD)
for (group in grouped_data) {
  # Perform LOOCV for each group
  loo_predictions <- c(loo_predictions, perform_loocv(group, "your_model.stan"))
}

# Calculate RMSE or MAE using loo_predictions and the true values
# Compare the performance of Bayesian Kriging vs Classical Kriging

```

```{r leave-one-out_loo_subsample}

# Assuming you have your distance matrix and crime_data prepared
stan_data <- list(N = nrow(crime_data),
                  y = crime_data$MONTH, 
                  distance_matrix = dist_matrix,
                  P = ncol(crime_data))


parameter_draws_1 <- extract(fit)$beta
stan_df_1 <- as.data.frame(stan_data)

# compute relative efficiency (this is slow and optional but is recommended to allow 
# for adjusting PSIS effective sample size based on MCMC effective sample size)
r_eff <- relative_eff(llfun_logistic, 
                      log = FALSE, # relative_eff wants likelihood not log-likelihood values
                      chain_id = rep(1:4, each = 1000), 
                      data = stan_df_1, 
                      draws = parameter_draws_1, 
                      cores = 2)
loo_i(i = 1, llfun_logistic, r_eff = r_eff, data = stan_df_1, draws = parameter_draws_1)

loo_ss_1 <- loo_subsample(
  crime_data_sf,
  observations = 100, # take a subsample of size 100
  cores = 2, 
  r_eff = r_eff,
  draws = parameter_draws_1,
  data = stan_df_1
)

print(loo_ss_1)

  
```


```{r}

#new locations 

# Example grid points
#new_locations <- expand.grid(x = seq(min(x_coords), max(x_coords), 
#                                     length.out = 50),
#                             y = seq(min(y_coords), max(y_coords), 
#                                     length.out = 50))

observed_locations <- st_coordinates(crime_data_sf)
new_locations <- st_coordinates(crime_data_sf_huh)
N_old <- nrow(crime_data)
N_new <- nrow(new_locations)

#dist matrix between new loc and exist
# Calculate distance matrix between new locations and existing data points
predict_distance_matrix <- as.matrix(dist(rbind(observed_locations, new_locations)))
predict_distance_matrix <- predict_distance_matrix[(N_old+1):(N_old+N_new), 1:N_old]  # Only new to old

```

```{r}

# Extract parameters from the fitted model
posterior_samples <- rstan::extract(fit)
mu_est <- mean(posterior_samples$mu)
range_est <- mean(posterior_samples$range)
sill_est <- mean(posterior_samples$sill)
nugget_est <- mean(posterior_samples$nugget)

# Prediction  #i dont understand #can i just pretend sigma doesnt exist
#Sigma_OO <- sill_est * exp(-distance_matrix / range_est) + diag(N_old) * nugget_est #these need to be generated in the model i thin
#Sigma_ON <- sill_est * exp(-predict_distance_matrix / range_est) #needs to be generated in the model 
predicted_means <- mu_est #+ Sigma_ON %*% solve(Sigma_OO, y - mu_est)

```

```{r}
#i dont think i know what I'm doing
new_data <- data.frame(new_locations, Z = as.vector(predicted_means))
ggplot(new_data, aes(x = X, y = Y, fill = Z)) +
  geom_tile() +
  scale_fill_viridis_c() +
  coord_fixed() +
  labs(title = "Spatial Interpolation Results", x = "X coordinate", y = "Y coordinate", fill = "Predicted Value")

```


